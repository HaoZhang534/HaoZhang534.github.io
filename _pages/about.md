---
permalink: /
title: About Me
excerpt: "Hao Zhang's Homepage"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I'm a 4th-year PhD (2020-now) student at the [Department of Computer Science & Engineering](https://cse.hkust.edu.hk/), [Hong Kong University of Science and Technology](https://hkust.edu.hk/), co-supervised by Prof. [Heung-Yeung Shum](https://scholar.google.com/citations?user=9akH-n8AAAAJ&hl=zh-CN) and Prof. [Lionel M. Ni](https://scholar.google.com/citations?user=OzMYwDIAAAAJ&hl=zh-CN). I interned at [International Digital Economy Academy, Shenzhen](https://idea.edu.cn/) (advised by Prof. [Lei Zhang](https://www.leizhang.org/)) and [Microsoft Research, Redmond](https://www.microsoft.com/en-us/research/group/deep-learning-group/) (advised by Dr. [Jianwei Yang](https://jwyang.github.io/) and Dr. [Chunyuan Li](https://chunyuan.li/)). 
Previously, I obtained my bachelor‚Äôs degree from School of Electronic Information and Electrical Engineering in Shanghai Jiao Tong University in 2019. 
<!-- I am always open to research discussions and collaborations. Feel free to get in touch! -->
<!-- 
**Research Interests** -->

üìåMy research interests lie in large multi-modal models, visual understanding and generation.

‚úâÔ∏è Welcome to contact me for any discussion and cooperation!


# üî• News
- \[2023/12\]: &nbsp; [Demo](https://llava-grounding.deepdataspace.com/) and [inference code](https://github.com/UX-Decoder/LLaVA-Grounding) of [LLaVA-Grounding](https://arxiv.org/abs/2312.02949) is released.
- \[2023/9\]: &nbsp;[Mask DINO](https://arxiv.org/pdf/2206.02777.pdf) ranks **9th** among [the most influential CVPR 2023 papers](https://www.paperdigest.org/2023/09/most-influential-cvpr-papers-2023-09/).
- \[2023/4\]: &nbsp;[DINO](https://arxiv.org/abs/2203.0360) ranks **2nd** among [the most influential ICLR 2023 papers](https://www.paperdigest.org/2023/04/most-influential-iclr-papers-2023-04/).
- \[2023/3\]: &nbsp;[DINO](https://arxiv.org/abs/2203.0360) and [DN-DETR](https://arxiv.org/pdf/2203.01305) are selected as [the top 100 most cited AI papers for 2022](https://www.zeta-alpha.com/post/must-read-the-100-most-cited-ai-papers-in-2022), rank 38 and 53, respectively.
- \[2023/3\]: &nbsp;Three papers accepted to CVPR 2023! Check out our [Mask DINO](https://arxiv.org/pdf/2206.02777.pdf) , [Lite DETR](https://arxiv.org/pdf/2303.07335.pdf), and [MP-Former](https://arxiv.org/pdf/2303.07336.pdf).
- \[2023/1\]: &nbsp;Two papers accepted to ICLR 2023! Check out our [DINO](https://arxiv.org/abs/2203.0360) and [ED-Pose](https://arxiv.org/pdf/2302.01593.pdf).
- \[2022/6\]: &nbsp;Checkout our unified detection and segmentation model [Mask DINO](https://arxiv.org/pdf/2206.02777.pdf) that achieves **SOTA** results on all the three segmentation tasks ([COCO instance](https://paperswithcode.com/sota/instance-segmentation-on-coco-minival), [COCO panoptic](https://paperswithcode.com/sota/panoptic-segmentation-on-coco-minival), and [ADE20K semantic](https://paperswithcode.com/sota/semantic-segmentation-on-ade20k))! Code is available [here](https://github.com/IDEACVR/MaskDINO).
- \[2022/3\]: &nbsp; We release [DINO](https://arxiv.org/abs/2203.03605) that for the first time establishes a DETR-like model as a **SOTA** model on the [COCO object detection leaderboard](https://paperswithcode.com/sota/object-detection-on-coco).  Code is available [here](https://github.com/IDEACVR/DINO).
- \[2022/3\]: &nbsp;Our [DN-DETR](https://arxiv.org/pdf/2203.01305) is selected for an **oral** presentation in **CVPR 2022**! Code is now available [here](https://github.com/IDEA-opensource/DN-DETR).


# üìù Recent Works
Refer to my [google scholar](https://scholar.google.com/citations?user=B8hPxMQAAAAJ&hl=en) for the full list.

<!-- <div class='paper-box'>
<div class='paper-box-text' markdown="1"> -->
* **LLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models**.  
**Hao Zhang\***, Hongyang Li\*, Feng Li, Tianhe Ren, Xueyan Zou, Shilong Liu, Shijia Huang, Jianfeng Gao, Lei Zhang, Chunyuan Li, Jianwei Yang.                
arxiv 2023.  
[[**Paper**]](https://arxiv.org/abs/2312.02949)[[**Code**]](https://github.com/UX-Decoder/LLaVA-Grounding)

* **Semantic-SAM: Segment and Recognize Anything at Any Granularity**.  
Feng Li\*, **Hao Zhang\***, Peize Sun, Xueyan Zou, Shilong Liu, Jianwei Yang, Chunyuan Li, Lei Zhang, Jianfeng Gao.                 
arxiv 2023.  
[[**Paper**]](https://arxiv.org/pdf/2307.04767.pdf)[[**Code**]](https://github.com/UX-Decoder/Semantic-SAM)

* **SoM: Set-of-Mark Visual Prompting for GPT-4V**.  
Jianwei Yang\*, **Hao Zhang\***,Feng Li\*, Xueyan Zou\*, Chunyuan Li, Jianfeng Gao.                 
arxiv 2023.  
[[**Paper**]](https://arxiv.org/pdf/2310.11441.pdf)[[**Code**]](https://github.com/microsoft/SoM)

* **OpenSeeD: A Simple Framework for Open-Vocabulary Segmentation and Detection**.  
**Hao Zhang\***, Feng Li\*, Xueyan Zou\*, Shilong Liu, Chunyuan Li, Jianfeng Gao, Jianwei Yang, Lei Zhang.
ICCV 2023.  
[[**Paper**]](https://arxiv.org/abs/2303.08131)[[**Code**]](https://github.com/IDEA-Research/OpenSeeD)

* **SEEM: Segment Everything Everywhere All at Once**.  
Xueyan Zou\*, Jianwei Yang\*, **Hao Zhang\***, Feng Li\*, Linjie Li, Jianfeng Gao, Yong Jae Lee.   
NeurIPS 2023.  
[[**Paper**]](https://arxiv.org/pdf/2304.06718.pdf)[[**Code**]](https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once)


* **Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection**.  
Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, **Hao Zhang**, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, Lei Zhang.   
arxiv 2023.  
[[**Paper**]](https://arxiv.org/abs/2303.05499)[[**Code**]](https://github.com/IDEA-Research/GroundingDINO)


* **Mask DINO: Towards A Unified Transformer-based Framework for Object Detection and Segmentation**.  
Feng Li\*, **Hao Zhang\***, Huaizhe Xu, Shilong Liu, Lei Zhang, Lionel M. Ni, Heung-Yeung Shum.   
CVPR 2023.  
[[**Paper**]](https://arxiv.org/pdf/2206.02777.pdf)[[**Code**]](https://github.com/IDEACVR/MaskDINO)

* **DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection**.  
**Hao Zhang\***, Feng Li\*, Shilong Liu\*, Lei Zhang, Hang Su, Jun Zhu, Lionel M. Ni, Heung-Yeung Shum.   
ICLR 2023.  
[[**Paper**]](https://arxiv.org/abs/2203.03605)[[**Code**]](https://github.com/IDEACVR/DINO) **Rank 2nd on ICLR 2023 Most Inflentical Papers**
  
* **DN-DETR: Accelerate DETR Training by Introducing Query DeNoising**.   
Feng Li\*, **Hao Zhang\***, Shilong Liu, Jian Guo, Lionel M. Ni, Lei Zhang.   
IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2022. **Oral** presentation.   
[[**Paper**]](https://arxiv.org/pdf/2203.01305)[[**Code**]](https://github.com/FengLi-ust/DN-DETR)
 

<!-- * BiCrowd: Online Bi-Objective Incentive Mechanism for Mobile Crowd Sensing.   
Yi-Fan Zhang, Xinglin Zhang, and **Feng Li**.   
IEEE Internet of Things Journal (JCR Q1).  
[[**Paper**]](https://fengli-ust.github.io/files/BiCrowd-IOT-J.pdf) -->

<!-- </div>
</div> -->

_(* denotes equal contribution.)_
# üéñ Selected Awards
* RedBird Research Scholarship in HKUST, 2020
* Hong Kong Postgraduate Scholoarship, 2020, 2021, 2022 and 2023

<!-- # üìñ Work experience
* March 2021 - Now: Research Assistant
  * Microsoft Research Asia, Beijing, China.
  * Duties included: 1. Design more powerful and simple object detection architecture based on the Transformer. 2. Understand NLP tasks such as NLI and exploit new paradigms to solve them more efficiently.
  * Advisor: Prof. [Jingdong Wang](https://jingdongwang2017.github.io/)

* August 2020 - Now: Research Assistant
  * University of Chinese Academy of Sciences, Beijing, China.
  * Duties included: 1. learning deep generative model for pedestrian generation. 2. cross-domain Re-ID from a causal view. 3. designing an efficient method to tackle problems in object detection and partial pedestrian re-identification.
  * Advisor: Prof. [Tieniu Tan](http://people.ucas.ac.cn/~tantieniu)
  * Co-Advisors: Prof. [Zhang Zhang](https://scholar.google.com/citations?user=rnRNwEMAAAAJ&hl=en) and Prof. [Liang Wang](https://scholar.google.com/citations?user=8kzzUboAAAAJ&hl=zh-CN)

* April 2018 ‚Äì July 2020: Research Assistant
  * South China University of Technology, Guangzhou, China.
  * Duties included: Incentive mechanism design for crowdsourcing platforms, edge computing
platforms, and federal learning platforms.
  * Advisor: Prof. Xinglin Zhang
 -->
<!-- # üí¨ Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/)

# üíª Internships
- *2019.05 - 2020.02*, [Lorem](https://github.com/), China. -->

<a href="http://s01.flagcounter.com/more/k4"><img src="https://s01.flagcounter.com/map/k4/size_s/txt_000000/border_CCCCCC/pageviews_0/viewers_0/flags_0/" alt="Flag Counter" border="0"></a>

<a href="https://info.flagcounter.com/PUlW"><img src="https://s01.flagcounter.com/mini/PUlW/bg_FFFFFF/txt_000000/border_CCCCCC/flags_0/" alt="Flag Counter" border="0"></a>

